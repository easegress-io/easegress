/*
 * Copyright (c) 2017, MegaEase
 * All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package layer4rawserver

import (
	"fmt"
	"net"
	"reflect"
	"sync/atomic"
	"time"

	"github.com/megaease/easegress/pkg/context"
	"github.com/megaease/easegress/pkg/logger"
	"github.com/megaease/easegress/pkg/protocol"
	"github.com/megaease/easegress/pkg/supervisor"
	"github.com/megaease/easegress/pkg/util/connection"
	"github.com/megaease/easegress/pkg/util/iobufferpool"
	"github.com/megaease/easegress/pkg/util/layer4stat"
)

const (
	checkFailedTimeout = 10 * time.Second

	stateNil     stateType = "nil"
	stateFailed  stateType = "failed"
	stateRunning stateType = "running"
	stateClosed  stateType = "closed"
)

var (
	errNil = fmt.Errorf("")
)

type (
	stateType string

	eventCheckFailed struct{}
	eventServeFailed struct {
		startNum uint64
		err      error
	}

	eventReload struct {
		nextSuperSpec *supervisor.Spec
		muxMapper     protocol.Layer4MuxMapper
	}
	eventClose struct{ done chan struct{} }

	runtime struct {
		superSpec *supervisor.Spec
		spec      *Spec

		state atomic.Value // runtime running state
		err   atomic.Value // runtime running error

		startNum  uint64           // runtime start num
		eventChan chan interface{} // receive traffic controller event

		mux  *mux // mux for layer4 pipeline
		pool *pool

		listener *listener
		tcpstat  *layer4stat.Layer4Stat
	}

	// Status contains all status generated by runtime, for displaying to users.
	Status struct {
		Health string `yaml:"health"`

		State stateType `yaml:"state"`
		Error string    `yaml:"error,omitempty"`
	}
)

func newRuntime(superSpec *supervisor.Spec, muxMapper protocol.Layer4MuxMapper) *runtime {
	r := &runtime{
		superSpec: superSpec,
		eventChan: make(chan interface{}, 10),
	}

	r.mux = newMux(muxMapper)
	r.setState(stateNil)
	r.setError(errNil)

	go r.fsm()
	go r.checkFailed()

	return r
}

func (r *runtime) Close() {
	done := make(chan struct{})
	r.eventChan <- &eventClose{done: done}
	<-done
}

// Status returns HTTPServer Status.
func (r *runtime) Status() *Status {
	health := r.getError().Error()

	return &Status{
		Health: health,
		State:  r.getState(),
		Error:  r.getError().Error(),
	}
}

// FSM is the finite-state-machine for the runtime.
func (r *runtime) fsm() {
	for e := range r.eventChan {
		switch e := e.(type) {
		case *eventCheckFailed:
			r.handleEventCheckFailed(e)
		case *eventServeFailed:
			r.handleEventServeFailed(e)
		case *eventReload:
			r.handleEventReload(e)
		case *eventClose:
			r.handleEventClose(e)
			// NOTE: We don't close hs.eventChan,
			// in case of panic of any other goroutines
			// to send event to it later.
			return
		default:
			logger.Errorf("BUG: unknown event: %T\n", e)
		}
	}
}

func (r *runtime) reload(nextSuperSpec *supervisor.Spec, muxMapper protocol.Layer4MuxMapper) {
	r.superSpec = nextSuperSpec
	r.mux.reloadRules(nextSuperSpec, muxMapper)

	nextSpec := nextSuperSpec.ObjectSpec().(*Spec)
	r.pool = newPool(nextSuperSpec.Super(), nextSpec.Pool, "")

	// r.listener does not create just after the process started and the config load for the first time.
	if nextSpec != nil && r.listener != nil {
		r.listener.setMaxConnection(nextSpec.MaxConnections)
	}

	// NOTE: Due to the mechanism of supervisor,
	// nextSpec must not be nil, just defensive programming here.
	switch {
	case r.spec == nil && nextSpec == nil:
		logger.Errorf("BUG: nextSpec is nil")
		// Nothing to do.
	case r.spec == nil && nextSpec != nil:
		r.spec = nextSpec
		r.startServer()
	case r.spec != nil && nextSpec == nil:
		logger.Errorf("BUG: nextSpec is nil")
		r.spec = nil
		r.closeServer()
	case r.spec != nil && nextSpec != nil:
		if r.needRestartServer(nextSpec) {
			r.spec = nextSpec
			r.closeServer()
			r.startServer()
		} else {
			r.spec = nextSpec
		}
	}
}

func (r *runtime) setState(state stateType) {
	r.state.Store(state)
}

func (r *runtime) getState() stateType {
	return r.state.Load().(stateType)
}

func (r *runtime) setError(err error) {
	if err == nil {
		r.err.Store(errNil)
	} else {
		// NOTE: For type safe.
		r.err.Store(fmt.Errorf("%v", err))
	}
}

func (r *runtime) getError() error {
	err := r.err.Load()
	if err == nil {
		return nil
	}
	return err.(error)
}

func (r *runtime) handleEventCheckFailed(e *eventCheckFailed) {

}

func (r *runtime) handleEventServeFailed(e *eventServeFailed) {
	if r.startNum > e.startNum {
		return
	}
	r.setState(stateFailed)
	r.setError(e.err)
}

func (r *runtime) handleEventReload(e *eventReload) {

}

func (r *runtime) handleEventClose(e *eventClose) {
	r.closeServer()
	close(e.done)
}

func (r *runtime) onTcpAccept() func(conn net.Conn, listenerStop chan struct{}) {
	if r.spec.Protocol != "tcp" {
		return nil
	}

	return func(conn net.Conn, listenerStop chan struct{}) {
		remote := conn.RemoteAddr().(*net.TCPAddr).IP.String()
		if r.mux.AllowIP(remote) {
			_ = conn.Close()
			logger.Infof("close tcp connection from %s to %s which ip is not allowed",
				conn.RemoteAddr().String(), conn.LocalAddr().String())
			return
		}

		server, err := r.pool.servers.next(remote)
		if err != nil {
			_ = conn.Close()
			logger.Errorf("close tcp connection due to can not find upstream server, local addr: %s, err: %+v",
				conn.LocalAddr(), err)
			return
		}

		stopChan := make(chan struct{})

		serverAddr, _ := net.ResolveTCPAddr("tcp", server.Addr)
		connTimeout := time.Duration(r.spec.ProxyConnectTimeout) * time.Millisecond
		upstreamConn := connection.NewUpstreamConn(connTimeout, serverAddr, listenerStop, stopChan)
		if err := upstreamConn.Connect(); err != nil {
			logger.Errorf("close tcp connection due to upstream conn connect failed, local addr: %s, err: %+v",
				conn.LocalAddr().String(), err)
			_ = conn.Close()
			close(stopChan)
			return
		}

		cliConn := connection.NewClientConn(conn, conn.RemoteAddr(), listenerStop, stopChan)
		ctx := context.NewLayer4Context(cliConn, upstreamConn, cliConn.RemoteAddr())
		r.setConnectionReadHandler(cliConn, upstreamConn, ctx)
	}
}

func (r *runtime) onUdpAccept() func(cliAddr net.Addr, conn net.Conn, listenerStop chan struct{}, buffer iobufferpool.IoBuffer) {
	if r.spec.Protocol != "udp" {
		return nil
	}

	return func(cliAddr net.Addr, conn net.Conn, listenerStop chan struct{}, buffer iobufferpool.IoBuffer) {
		localAddr := conn.LocalAddr()
		remote := cliAddr.(*net.UDPAddr).IP.String()
		if r.mux.AllowIP(remote) {
			logger.Infof("discard udp packet from %s to %s which ip is not allowed", cliAddr.String(), localAddr.String())
			return
		}

		key := connection.GetProxyMapKey(localAddr.String(), cliAddr.String())
		if rawCtx, ok := connection.ProxyMap.Load(key); ok {
			ctx := rawCtx.(context.Layer4Context)
			ctx.WriteToUpstream(buffer.Clone()) // there is no need to reset buffer
			return
		}

		server, err := r.pool.servers.next(remote)
		if err != nil {
			logger.Infof("discard udp packet from %s to %s due to can not find upstream server, err: %+v",
				cliAddr.String(), localAddr.String())
			return
		}

		upstreamAddr, _ := net.ResolveUDPAddr("udp", server.Addr)
		connTimeout := time.Duration(r.spec.ProxyConnectTimeout) * time.Millisecond
		upstreamConn := connection.NewUpstreamConn(connTimeout, upstreamAddr, listenerStop, nil)
		if err := upstreamConn.Connect(); err != nil {
			logger.Errorf("discard udp packet due to upstream connect failed, local addr: %s, err: %+v", localAddr, err)
			return
		}

		cliConn := connection.NewClientConn(conn, conn.RemoteAddr(), listenerStop, nil)
		ctx := context.NewLayer4Context(cliConn, upstreamConn, cliAddr)
		connection.SetUDPProxyMap(connection.GetProxyMapKey(localAddr.String(), cliAddr.String()), ctx)
		r.setConnectionReadHandler(cliConn, upstreamConn, ctx)
	}
}

func (r *runtime) setConnectionReadHandler(cliConn *connection.Connection, upstreamConn *connection.UpstreamConnection, ctx context.Layer4Context) {
	if handle, ok := r.mux.GetHandler(r.spec.Name); ok {
		cliConn.SetOnRead(func(buffer iobufferpool.IoBuffer) {
			handle.InboundHandler(ctx, buffer)
			if ctx.GetReadBuffer().Len() > 0 {
				buf := ctx.GetReadBuffer().Clone()
				ctx.GetReadBuffer().Reset()
				ctx.WriteToUpstream(buf)
			}
		})
		upstreamConn.SetOnRead(func(buffer iobufferpool.IoBuffer) {
			handle.OutboundHandler(ctx, buffer)
			if ctx.GetReadBuffer().Len() > 0 {
				buf := ctx.GetReadBuffer().Clone()
				ctx.GetReadBuffer().Reset()
				ctx.WriteToClient(buf)
			}
		})
	} else {
		cliConn.SetOnRead(func(buffer iobufferpool.IoBuffer) {
			ctx.WriteToUpstream(buffer.Clone())
			buffer.Reset()
		})
		upstreamConn.SetOnRead(func(buffer iobufferpool.IoBuffer) {
			ctx.WriteToClient(buffer.Clone())
			buffer.Reset()
		})
	}
}

func (r *runtime) startServer() {
	l := newListener(r.spec, r.onTcpAccept(), r.onUdpAccept())
	err := l.listen()
	if err != nil {
		r.setState(stateFailed)
		r.setError(err)
		logger.Errorf("listen tcp conn for :%d failed, err: %v", r.spec.Port, err)

		_ = l.close()
		r.eventChan <- &eventServeFailed{
			err:      err,
			startNum: r.startNum,
		}
		return
	}

	r.startNum++
	r.setState(stateRunning)
	r.setError(nil)

	r.listener = l
	go r.listener.start()
}

func (r *runtime) closeServer() {
	_ = r.listener.close() // TODO close established connection when listener closed?
	logger.Infof("listener for %s :%d closed,")
}

func (r *runtime) checkFailed() {
	ticker := time.NewTicker(checkFailedTimeout)
	for range ticker.C {
		state := r.getState()
		if state == stateFailed {
			r.eventChan <- &eventCheckFailed{}
		} else if state == stateClosed {
			ticker.Stop()
			return
		}
	}
}

func (r *runtime) needRestartServer(nextSpec *Spec) bool {
	x := *r.spec
	y := *nextSpec

	// The change of options below need not restart the HTTP server.
	x.MaxConnections, y.MaxConnections = 0, 0
	x.IPFilter, y.IPFilter = nil, nil
	x.Pool, y.Pool = nil, nil
	x.ProxyConnectTimeout, y.ProxyTimeout = 0, 0
	x.ProxyTimeout, y.ProxyTimeout = 0, 0

	// The update of rules need not to shutdown server.
	return !reflect.DeepEqual(x, y)
}
